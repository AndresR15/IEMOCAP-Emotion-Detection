{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Merge,Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "from features import *\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = code_path + \"/../data/sessions/\"\n",
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "framerate = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(data_path + '/../'+'data_collected.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(frames, freq, options):\n",
    "    window_sec = 0.2\n",
    "    window_n = int(freq * window_sec)\n",
    "\n",
    "    st_f = stFeatureExtraction(frames, freq, window_n, window_n / 2)\n",
    "\n",
    "    if st_f.shape[1] > 2:\n",
    "        i0 = 1\n",
    "        i1 = st_f.shape[1] - 1\n",
    "        if i1 - i0 < 1:\n",
    "            i1 = i0 + 1\n",
    "        \n",
    "        deriv_st_f = np.zeros((st_f.shape[0], i1 - i0), dtype=float)\n",
    "        for i in range(i0, i1):\n",
    "            i_left = i - 1\n",
    "            i_right = i + 1\n",
    "            deriv_st_f[:st_f.shape[0], i - i0] = st_f[:, i]\n",
    "        return deriv_st_f\n",
    "    elif st_f.shape[1] == 2:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f\n",
    "    else:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4936, 100, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_speech = []\n",
    "\n",
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    x_head = ses_mod['signal']\n",
    "    st_features = calculate_features(x_head, framerate, None)\n",
    "    st_features, _ = pad_sequence_into_array(st_features, maxlen=100)\n",
    "    x_train_speech.append( st_features.T )\n",
    "    counter+=1\n",
    "    if(counter%100==0):\n",
    "        print(counter)\n",
    "    \n",
    "x_train_speech = np.array(x_train_speech)\n",
    "x_train_speech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(100, 34)))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1120256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,041,348\n",
      "Trainable params: 2,041,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=[]\n",
    "for ses_mod in data2:\n",
    "    Y.append(ses_mod['emotion'])\n",
    "    \n",
    "Y = label_binarize(Y,emotions_used)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/60\n",
      "3948/3948 [==============================] - 17s 4ms/step - loss: 1.3657 - acc: 0.3544 - val_loss: 1.3529 - val_acc: 0.3826\n",
      "Epoch 2/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3540 - acc: 0.3599 - val_loss: 1.3603 - val_acc: 0.3846\n",
      "Epoch 3/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3543 - acc: 0.3592 - val_loss: 1.3518 - val_acc: 0.3796\n",
      "Epoch 4/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3509 - acc: 0.3622 - val_loss: 1.3451 - val_acc: 0.3897\n",
      "Epoch 5/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3462 - acc: 0.3690 - val_loss: 1.3748 - val_acc: 0.3866\n",
      "Epoch 6/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3511 - acc: 0.3668 - val_loss: 1.3332 - val_acc: 0.3947\n",
      "Epoch 7/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3477 - acc: 0.3668 - val_loss: 1.3518 - val_acc: 0.4018\n",
      "Epoch 8/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3466 - acc: 0.3612 - val_loss: 1.3561 - val_acc: 0.2672\n",
      "Epoch 9/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3279 - acc: 0.3726 - val_loss: 1.4658 - val_acc: 0.2075\n",
      "Epoch 10/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.3429 - acc: 0.3660 - val_loss: 1.2594 - val_acc: 0.4302\n",
      "Epoch 11/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.2508 - acc: 0.4154 - val_loss: 1.3937 - val_acc: 0.3998\n",
      "Epoch 12/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.2250 - acc: 0.4212 - val_loss: 1.2808 - val_acc: 0.4332\n",
      "Epoch 13/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1966 - acc: 0.4255 - val_loss: 1.3268 - val_acc: 0.3229\n",
      "Epoch 14/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1902 - acc: 0.4352 - val_loss: 1.3093 - val_acc: 0.4038\n",
      "Epoch 15/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1944 - acc: 0.4329 - val_loss: 1.4394 - val_acc: 0.3026\n",
      "Epoch 16/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1850 - acc: 0.4433 - val_loss: 1.2621 - val_acc: 0.4281\n",
      "Epoch 17/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1593 - acc: 0.4554 - val_loss: 1.3361 - val_acc: 0.3796\n",
      "Epoch 18/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1612 - acc: 0.4607 - val_loss: 1.3766 - val_acc: 0.4241\n",
      "Epoch 19/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1511 - acc: 0.4653 - val_loss: 1.1961 - val_acc: 0.4484\n",
      "Epoch 20/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1431 - acc: 0.4643 - val_loss: 1.3068 - val_acc: 0.3573\n",
      "Epoch 21/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1383 - acc: 0.4671 - val_loss: 1.2298 - val_acc: 0.4656\n",
      "Epoch 22/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1371 - acc: 0.4686 - val_loss: 1.2642 - val_acc: 0.4555\n",
      "Epoch 23/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1249 - acc: 0.4737 - val_loss: 1.1706 - val_acc: 0.4808\n",
      "Epoch 24/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1261 - acc: 0.4780 - val_loss: 1.1833 - val_acc: 0.5132\n",
      "Epoch 25/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1323 - acc: 0.4785 - val_loss: 1.1479 - val_acc: 0.4615\n",
      "Epoch 26/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1159 - acc: 0.4904 - val_loss: 1.2885 - val_acc: 0.4099\n",
      "Epoch 27/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1276 - acc: 0.4828 - val_loss: 1.1615 - val_acc: 0.4706\n",
      "Epoch 28/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1178 - acc: 0.4770 - val_loss: 1.1628 - val_acc: 0.4717\n",
      "Epoch 29/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1058 - acc: 0.4777 - val_loss: 1.2987 - val_acc: 0.4494\n",
      "Epoch 30/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0981 - acc: 0.4914 - val_loss: 1.1502 - val_acc: 0.4767\n",
      "Epoch 31/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.1028 - acc: 0.4889 - val_loss: 1.2561 - val_acc: 0.4069\n",
      "Epoch 32/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0962 - acc: 0.4957 - val_loss: 1.1428 - val_acc: 0.4960\n",
      "Epoch 33/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0958 - acc: 0.4878 - val_loss: 1.3586 - val_acc: 0.3826\n",
      "Epoch 34/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0955 - acc: 0.4932 - val_loss: 1.1425 - val_acc: 0.4494\n",
      "Epoch 35/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0877 - acc: 0.4889 - val_loss: 1.1168 - val_acc: 0.4818\n",
      "Epoch 36/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0771 - acc: 0.5053 - val_loss: 1.1481 - val_acc: 0.4798\n",
      "Epoch 37/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0790 - acc: 0.5033 - val_loss: 1.2553 - val_acc: 0.4130\n",
      "Epoch 38/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0792 - acc: 0.4992 - val_loss: 1.2329 - val_acc: 0.4636\n",
      "Epoch 39/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0767 - acc: 0.4949 - val_loss: 1.4115 - val_acc: 0.3816\n",
      "Epoch 40/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0761 - acc: 0.5063 - val_loss: 1.1770 - val_acc: 0.4757\n",
      "Epoch 41/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0666 - acc: 0.5134 - val_loss: 1.1847 - val_acc: 0.4686\n",
      "Epoch 42/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0542 - acc: 0.5091 - val_loss: 1.3735 - val_acc: 0.3603\n",
      "Epoch 43/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0566 - acc: 0.5127 - val_loss: 1.2336 - val_acc: 0.4241\n",
      "Epoch 44/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0526 - acc: 0.5155 - val_loss: 1.1871 - val_acc: 0.4443\n",
      "Epoch 45/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0518 - acc: 0.5096 - val_loss: 1.1863 - val_acc: 0.4656\n",
      "Epoch 46/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0419 - acc: 0.5195 - val_loss: 1.1526 - val_acc: 0.4717\n",
      "Epoch 47/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0395 - acc: 0.5185 - val_loss: 1.1266 - val_acc: 0.4889\n",
      "Epoch 48/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0312 - acc: 0.5142 - val_loss: 1.2535 - val_acc: 0.4211\n",
      "Epoch 49/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0297 - acc: 0.5329 - val_loss: 1.1327 - val_acc: 0.4889\n",
      "Epoch 50/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0229 - acc: 0.5263 - val_loss: 1.2477 - val_acc: 0.4453\n",
      "Epoch 51/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0261 - acc: 0.5291 - val_loss: 1.1889 - val_acc: 0.4919\n",
      "Epoch 52/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0213 - acc: 0.5314 - val_loss: 1.1888 - val_acc: 0.4777\n",
      "Epoch 53/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0160 - acc: 0.5309 - val_loss: 1.1447 - val_acc: 0.4939\n",
      "Epoch 54/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0050 - acc: 0.5415 - val_loss: 1.1152 - val_acc: 0.5051\n",
      "Epoch 55/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 1.0049 - acc: 0.5436 - val_loss: 1.1594 - val_acc: 0.4534\n",
      "Epoch 56/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 0.9991 - acc: 0.5423 - val_loss: 1.1467 - val_acc: 0.4970\n",
      "Epoch 57/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 0.9859 - acc: 0.5519 - val_loss: 1.1653 - val_acc: 0.4787\n",
      "Epoch 58/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 0.9876 - acc: 0.5532 - val_loss: 1.1422 - val_acc: 0.5010\n",
      "Epoch 59/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 0.9761 - acc: 0.5580 - val_loss: 1.1346 - val_acc: 0.5000\n",
      "Epoch 60/60\n",
      "3948/3948 [==============================] - 16s 4ms/step - loss: 0.9625 - acc: 0.5618 - val_loss: 1.1816 - val_acc: 0.4747\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=60, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaldi_model(optimizer='Adam'):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Dense(450), input_shape=(100, 34)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(TimeDistributed(Dense(450)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(TimeDistributed(Dense(450)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(TimeDistributed(Dense(450)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(TimeDistributed(Dense(450)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(TimeDistributed(Dense(450)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 100, 450)          15750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 450)          1800      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100, 450)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 100, 450)          202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 450)          1800      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100, 450)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 100, 450)          202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100, 450)          1800      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100, 450)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 100, 450)          202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100, 450)          1800      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100, 450)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 100, 450)          202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100, 450)          1800      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100, 450)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 100, 450)          202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100, 450)          1800      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100, 450)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 45000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               4500100   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 404       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 5,541,804\n",
      "Trainable params: 5,536,404\n",
      "Non-trainable params: 5,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kaldi_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/40\n",
      "3948/3948 [==============================] - 5s 1ms/step - loss: 11.9052 - acc: 0.2394 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 2/40\n",
      "3948/3948 [==============================] - 4s 986us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 3/40\n",
      "3948/3948 [==============================] - 4s 987us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 4/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 5/40\n",
      "3948/3948 [==============================] - 4s 987us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 6/40\n",
      "3948/3948 [==============================] - 4s 988us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 7/40\n",
      "3948/3948 [==============================] - 4s 988us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 8/40\n",
      "3948/3948 [==============================] - 4s 991us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 9/40\n",
      "3948/3948 [==============================] - 4s 989us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 10/40\n",
      "3948/3948 [==============================] - 4s 989us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 11/40\n",
      "3948/3948 [==============================] - 4s 991us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 12/40\n",
      "3948/3948 [==============================] - 4s 989us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 13/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 14/40\n",
      "3948/3948 [==============================] - 4s 994us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 15/40\n",
      "3948/3948 [==============================] - 4s 997us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 16/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 17/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 18/40\n",
      "3948/3948 [==============================] - 4s 991us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 19/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 20/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 21/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 22/40\n",
      "3948/3948 [==============================] - 4s 989us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 23/40\n",
      "3948/3948 [==============================] - 4s 991us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 24/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 25/40\n",
      "3948/3948 [==============================] - 4s 989us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 26/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 27/40\n",
      "3948/3948 [==============================] - 4s 993us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 28/40\n",
      "3948/3948 [==============================] - 4s 991us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 29/40\n",
      "3948/3948 [==============================] - 4s 993us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 30/40\n",
      "3948/3948 [==============================] - 4s 989us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 31/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 32/40\n",
      "3948/3948 [==============================] - 4s 994us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 33/40\n",
      "3948/3948 [==============================] - 4s 987us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 34/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 35/40\n",
      "3948/3948 [==============================] - 4s 985us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 36/40\n",
      "3948/3948 [==============================] - 4s 990us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 37/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 38/40\n",
      "3948/3948 [==============================] - 4s 992us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 39/40\n",
      "3948/3948 [==============================] - 4s 995us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n",
      "Epoch 40/40\n",
      "3948/3948 [==============================] - 4s 991us/step - loss: 12.2723 - acc: 0.2386 - val_loss: 13.4916 - val_acc: 0.1630\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=40, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(100, 34)))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 3400)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              3482624   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 4,139,780\n",
      "Trainable params: 4,139,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = linear_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/80\n",
      "3948/3948 [==============================] - 1s 243us/step - loss: 1.7506 - acc: 0.3488 - val_loss: 1.3135 - val_acc: 0.3937\n",
      "Epoch 2/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 1.2712 - acc: 0.4040 - val_loss: 1.3011 - val_acc: 0.3887\n",
      "Epoch 3/80\n",
      "3948/3948 [==============================] - 0s 127us/step - loss: 1.1822 - acc: 0.4607 - val_loss: 1.4737 - val_acc: 0.3107\n",
      "Epoch 4/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 1.1902 - acc: 0.4488 - val_loss: 1.2535 - val_acc: 0.4038\n",
      "Epoch 5/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 1.1178 - acc: 0.4876 - val_loss: 1.5948 - val_acc: 0.3715\n",
      "Epoch 6/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 1.1082 - acc: 0.4934 - val_loss: 1.4316 - val_acc: 0.3998\n",
      "Epoch 7/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 1.0682 - acc: 0.5301 - val_loss: 1.3327 - val_acc: 0.4059\n",
      "Epoch 8/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 1.0636 - acc: 0.5294 - val_loss: 1.3118 - val_acc: 0.4534\n",
      "Epoch 9/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 1.0336 - acc: 0.5446 - val_loss: 1.1888 - val_acc: 0.4980\n",
      "Epoch 10/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 1.0124 - acc: 0.5436 - val_loss: 1.2926 - val_acc: 0.4818\n",
      "Epoch 11/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 1.0166 - acc: 0.5578 - val_loss: 1.5233 - val_acc: 0.3978\n",
      "Epoch 12/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.9935 - acc: 0.5648 - val_loss: 1.2401 - val_acc: 0.4990\n",
      "Epoch 13/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.9485 - acc: 0.5869 - val_loss: 1.2349 - val_acc: 0.4494\n",
      "Epoch 14/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.9433 - acc: 0.5879 - val_loss: 1.3359 - val_acc: 0.4464\n",
      "Epoch 15/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.9067 - acc: 0.6051 - val_loss: 1.3062 - val_acc: 0.4433\n",
      "Epoch 16/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.8866 - acc: 0.6213 - val_loss: 1.2340 - val_acc: 0.4868\n",
      "Epoch 17/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.8902 - acc: 0.6221 - val_loss: 1.3220 - val_acc: 0.4747\n",
      "Epoch 18/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.8623 - acc: 0.6292 - val_loss: 1.4366 - val_acc: 0.4585\n",
      "Epoch 19/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.8321 - acc: 0.6469 - val_loss: 1.3267 - val_acc: 0.4889\n",
      "Epoch 20/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.8136 - acc: 0.6593 - val_loss: 1.3389 - val_acc: 0.4261\n",
      "Epoch 21/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.8002 - acc: 0.6710 - val_loss: 1.2766 - val_acc: 0.4706\n",
      "Epoch 22/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.7826 - acc: 0.6745 - val_loss: 1.3818 - val_acc: 0.4889\n",
      "Epoch 23/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.7720 - acc: 0.6819 - val_loss: 1.2609 - val_acc: 0.4929\n",
      "Epoch 24/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.7116 - acc: 0.7062 - val_loss: 1.5116 - val_acc: 0.4383\n",
      "Epoch 25/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.7124 - acc: 0.7090 - val_loss: 1.4195 - val_acc: 0.4717\n",
      "Epoch 26/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.6703 - acc: 0.7183 - val_loss: 1.2470 - val_acc: 0.5061\n",
      "Epoch 27/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 0.6403 - acc: 0.7353 - val_loss: 1.4365 - val_acc: 0.4818\n",
      "Epoch 28/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.6128 - acc: 0.7492 - val_loss: 1.5584 - val_acc: 0.4605\n",
      "Epoch 29/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.5927 - acc: 0.7627 - val_loss: 1.3602 - val_acc: 0.4828\n",
      "Epoch 30/80\n",
      "3948/3948 [==============================] - 1s 130us/step - loss: 0.5841 - acc: 0.7670 - val_loss: 1.5516 - val_acc: 0.4474\n",
      "Epoch 31/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.5288 - acc: 0.7936 - val_loss: 1.5390 - val_acc: 0.4362\n",
      "Epoch 32/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 0.5076 - acc: 0.8014 - val_loss: 1.4795 - val_acc: 0.4636\n",
      "Epoch 33/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.4558 - acc: 0.8222 - val_loss: 1.7228 - val_acc: 0.4848\n",
      "Epoch 34/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.4895 - acc: 0.8078 - val_loss: 1.4430 - val_acc: 0.4939\n",
      "Epoch 35/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.4686 - acc: 0.8204 - val_loss: 1.5180 - val_acc: 0.4646\n",
      "Epoch 36/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.4328 - acc: 0.8427 - val_loss: 1.7100 - val_acc: 0.4423\n",
      "Epoch 37/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.3522 - acc: 0.8749 - val_loss: 2.0168 - val_acc: 0.4666\n",
      "Epoch 38/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.3636 - acc: 0.8574 - val_loss: 2.1110 - val_acc: 0.4646\n",
      "Epoch 39/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.3832 - acc: 0.8533 - val_loss: 1.5637 - val_acc: 0.4919\n",
      "Epoch 40/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.2950 - acc: 0.8893 - val_loss: 1.8661 - val_acc: 0.4605\n",
      "Epoch 41/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.2697 - acc: 0.9012 - val_loss: 1.8522 - val_acc: 0.4828\n",
      "Epoch 42/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 0.2910 - acc: 0.8906 - val_loss: 1.6353 - val_acc: 0.4494\n",
      "Epoch 43/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.3034 - acc: 0.8832 - val_loss: 1.6645 - val_acc: 0.4696\n",
      "Epoch 44/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.2404 - acc: 0.9116 - val_loss: 1.9658 - val_acc: 0.4980\n",
      "Epoch 45/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.3080 - acc: 0.8868 - val_loss: 1.8017 - val_acc: 0.4575\n",
      "Epoch 46/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.1807 - acc: 0.9372 - val_loss: 2.1741 - val_acc: 0.4383\n",
      "Epoch 47/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1916 - acc: 0.9281 - val_loss: 2.1914 - val_acc: 0.5000\n",
      "Epoch 48/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.2641 - acc: 0.9207 - val_loss: 2.0256 - val_acc: 0.4777\n",
      "Epoch 49/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 0.1747 - acc: 0.9326 - val_loss: 2.1436 - val_acc: 0.4717\n",
      "Epoch 50/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1839 - acc: 0.9326 - val_loss: 2.0418 - val_acc: 0.4524\n",
      "Epoch 51/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1321 - acc: 0.9559 - val_loss: 2.4693 - val_acc: 0.4757\n",
      "Epoch 52/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1297 - acc: 0.9552 - val_loss: 2.7066 - val_acc: 0.4737\n",
      "Epoch 53/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.2075 - acc: 0.9303 - val_loss: 2.7049 - val_acc: 0.4413\n",
      "Epoch 54/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.2248 - acc: 0.9354 - val_loss: 2.3138 - val_acc: 0.4808\n",
      "Epoch 55/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0762 - acc: 0.9807 - val_loss: 2.3922 - val_acc: 0.5020\n",
      "Epoch 56/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1684 - acc: 0.9438 - val_loss: 2.3383 - val_acc: 0.4828\n",
      "Epoch 57/80\n",
      "3948/3948 [==============================] - 0s 127us/step - loss: 0.0819 - acc: 0.9757 - val_loss: 2.9894 - val_acc: 0.4362\n",
      "Epoch 58/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.2572 - acc: 0.9281 - val_loss: 2.3537 - val_acc: 0.4737\n",
      "Epoch 59/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0714 - acc: 0.9772 - val_loss: 2.4514 - val_acc: 0.4686\n",
      "Epoch 60/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0826 - acc: 0.9706 - val_loss: 2.7730 - val_acc: 0.4848\n",
      "Epoch 61/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0899 - acc: 0.9729 - val_loss: 2.7245 - val_acc: 0.4848\n",
      "Epoch 62/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.2417 - acc: 0.9278 - val_loss: 2.3505 - val_acc: 0.4990\n",
      "Epoch 63/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0492 - acc: 0.9871 - val_loss: 2.8490 - val_acc: 0.4747\n",
      "Epoch 64/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0533 - acc: 0.9810 - val_loss: 2.7655 - val_acc: 0.4696\n",
      "Epoch 65/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.2827 - acc: 0.9144 - val_loss: 2.1670 - val_acc: 0.4686\n",
      "Epoch 66/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0499 - acc: 0.9904 - val_loss: 2.7631 - val_acc: 0.4818\n",
      "Epoch 67/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0368 - acc: 0.9904 - val_loss: 2.7832 - val_acc: 0.4767\n",
      "Epoch 68/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1377 - acc: 0.9569 - val_loss: 2.7128 - val_acc: 0.3846\n",
      "Epoch 69/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0841 - acc: 0.9772 - val_loss: 2.7999 - val_acc: 0.4970\n",
      "Epoch 70/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0407 - acc: 0.9881 - val_loss: 2.9428 - val_acc: 0.4949\n",
      "Epoch 71/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1700 - acc: 0.9488 - val_loss: 3.3459 - val_acc: 0.4464\n",
      "Epoch 72/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0697 - acc: 0.9835 - val_loss: 2.7705 - val_acc: 0.4757\n",
      "Epoch 73/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0331 - acc: 0.9899 - val_loss: 3.0065 - val_acc: 0.4787\n",
      "Epoch 74/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0645 - acc: 0.9792 - val_loss: 3.4684 - val_acc: 0.4626\n",
      "Epoch 75/80\n",
      "3948/3948 [==============================] - 1s 129us/step - loss: 0.0557 - acc: 0.9861 - val_loss: 3.0356 - val_acc: 0.4737\n",
      "Epoch 76/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0520 - acc: 0.9823 - val_loss: 3.2993 - val_acc: 0.4383\n",
      "Epoch 77/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.2664 - acc: 0.9354 - val_loss: 2.6608 - val_acc: 0.4929\n",
      "Epoch 78/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0206 - acc: 0.9959 - val_loss: 2.9269 - val_acc: 0.4858\n",
      "Epoch 79/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0519 - acc: 0.9851 - val_loss: 3.5575 - val_acc: 0.4291\n",
      "Epoch 80/80\n",
      "3948/3948 [==============================] - 1s 128us/step - loss: 0.0801 - acc: 0.9792 - val_loss: 2.9229 - val_acc: 0.4848\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=80, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_2(frames, freq, options):\n",
    "    #double the window duration\n",
    "    window_sec = 0.4\n",
    "    window_n = int(freq * window_sec)\n",
    "\n",
    "    st_f = stFeatureExtraction(frames, freq, window_n, window_n / 2)\n",
    "\n",
    "    if st_f.shape[1] > 2:\n",
    "        i0 = 1\n",
    "        i1 = st_f.shape[1] - 1\n",
    "        if i1 - i0 < 1:\n",
    "            i1 = i0 + 1\n",
    "        \n",
    "        deriv_st_f = np.zeros((st_f.shape[0], i1 - i0), dtype=float)\n",
    "        for i in range(i0, i1):\n",
    "            i_left = i - 1\n",
    "            i_right = i + 1\n",
    "            deriv_st_f[:st_f.shape[0], i - i0] = st_f[:, i]\n",
    "        return deriv_st_f\n",
    "    elif st_f.shape[1] == 2:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f\n",
    "    else:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4936, 100, 34)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_speech2 = []\n",
    "from sklearn.preprocessing import normalize\n",
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    x_head = ses_mod['signal']\n",
    "    st_features = calculate_features_2(x_head, framerate, None)\n",
    "    st_features, _ = pad_sequence_into_array(st_features, maxlen=100)\n",
    "    x_train_speech2.append( st_features.T )\n",
    "    counter+=1\n",
    "    if(counter%100==0):\n",
    "        print(counter)\n",
    "    \n",
    "x_train_speech2 = np.array(x_train_speech2)\n",
    "x_train_speech2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(100, 34)))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 3400)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              3482624   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 4,139,780\n",
      "Trainable params: 4,139,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = linear_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/80\n",
      "3948/3948 [==============================] - 1s 233us/step - loss: 1.5244 - acc: 0.3579 - val_loss: 1.3801 - val_acc: 0.4028\n",
      "Epoch 2/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 1.2189 - acc: 0.4412 - val_loss: 1.2390 - val_acc: 0.4302\n",
      "Epoch 3/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 1.1846 - acc: 0.4673 - val_loss: 1.2340 - val_acc: 0.4150\n",
      "Epoch 4/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 1.1283 - acc: 0.4845 - val_loss: 1.3704 - val_acc: 0.4211\n",
      "Epoch 5/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 1.0999 - acc: 0.4919 - val_loss: 1.2273 - val_acc: 0.4767\n",
      "Epoch 6/80\n",
      "3948/3948 [==============================] - 0s 124us/step - loss: 1.0920 - acc: 0.5043 - val_loss: 1.4274 - val_acc: 0.3816\n",
      "Epoch 7/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 1.0660 - acc: 0.5144 - val_loss: 1.3339 - val_acc: 0.4808\n",
      "Epoch 8/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 1.0435 - acc: 0.5304 - val_loss: 1.2034 - val_acc: 0.4939\n",
      "Epoch 9/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 1.0330 - acc: 0.5466 - val_loss: 1.2791 - val_acc: 0.4423\n",
      "Epoch 10/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9999 - acc: 0.5618 - val_loss: 1.3054 - val_acc: 0.4312\n",
      "Epoch 11/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9762 - acc: 0.5659 - val_loss: 1.5528 - val_acc: 0.3785\n",
      "Epoch 12/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9632 - acc: 0.5800 - val_loss: 1.7038 - val_acc: 0.3623\n",
      "Epoch 13/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9547 - acc: 0.5821 - val_loss: 1.2882 - val_acc: 0.4818\n",
      "Epoch 14/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9379 - acc: 0.5902 - val_loss: 1.3335 - val_acc: 0.4413\n",
      "Epoch 15/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9116 - acc: 0.6071 - val_loss: 1.2289 - val_acc: 0.4666\n",
      "Epoch 16/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.9019 - acc: 0.6039 - val_loss: 1.2413 - val_acc: 0.4909\n",
      "Epoch 17/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.8720 - acc: 0.6201 - val_loss: 1.2937 - val_acc: 0.4828\n",
      "Epoch 18/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.8795 - acc: 0.6246 - val_loss: 1.3328 - val_acc: 0.4737\n",
      "Epoch 19/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.8340 - acc: 0.6403 - val_loss: 1.4082 - val_acc: 0.4757\n",
      "Epoch 20/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.8232 - acc: 0.6479 - val_loss: 1.3590 - val_acc: 0.4919\n",
      "Epoch 21/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.8004 - acc: 0.6613 - val_loss: 1.3931 - val_acc: 0.4696\n",
      "Epoch 22/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.7932 - acc: 0.6654 - val_loss: 1.4697 - val_acc: 0.4362\n",
      "Epoch 23/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.7593 - acc: 0.6722 - val_loss: 1.3234 - val_acc: 0.5101\n",
      "Epoch 24/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.7289 - acc: 0.6945 - val_loss: 1.4128 - val_acc: 0.4808\n",
      "Epoch 25/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.6923 - acc: 0.7123 - val_loss: 1.6444 - val_acc: 0.4514\n",
      "Epoch 26/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.6818 - acc: 0.7120 - val_loss: 1.4650 - val_acc: 0.4757\n",
      "Epoch 27/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.6912 - acc: 0.7161 - val_loss: 1.6422 - val_acc: 0.3907\n",
      "Epoch 28/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.6420 - acc: 0.7442 - val_loss: 1.4990 - val_acc: 0.4909\n",
      "Epoch 29/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.6054 - acc: 0.7563 - val_loss: 1.4849 - val_acc: 0.4858\n",
      "Epoch 30/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.6000 - acc: 0.7604 - val_loss: 1.6073 - val_acc: 0.4787\n",
      "Epoch 31/80\n",
      "3948/3948 [==============================] - 0s 123us/step - loss: 0.5650 - acc: 0.7761 - val_loss: 1.6836 - val_acc: 0.4919\n",
      "Epoch 32/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.5672 - acc: 0.7657 - val_loss: 1.4524 - val_acc: 0.4706\n",
      "Epoch 33/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.5099 - acc: 0.7976 - val_loss: 1.6865 - val_acc: 0.4393\n",
      "Epoch 34/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.4950 - acc: 0.8002 - val_loss: 1.5096 - val_acc: 0.4889\n",
      "Epoch 35/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.4963 - acc: 0.8133 - val_loss: 1.8194 - val_acc: 0.4818\n",
      "Epoch 36/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.4319 - acc: 0.8371 - val_loss: 1.6395 - val_acc: 0.4747\n",
      "Epoch 37/80\n",
      "3948/3948 [==============================] - 0s 122us/step - loss: 0.4430 - acc: 0.8247 - val_loss: 1.6709 - val_acc: 0.4575\n",
      "Epoch 38/80\n",
      "3948/3948 [==============================] - 0s 124us/step - loss: 0.4161 - acc: 0.8432 - val_loss: 1.7798 - val_acc: 0.4453\n",
      "Epoch 39/80\n",
      "3948/3948 [==============================] - 0s 124us/step - loss: 0.4019 - acc: 0.8470 - val_loss: 1.6402 - val_acc: 0.4656\n",
      "Epoch 40/80\n",
      "3948/3948 [==============================] - 0s 124us/step - loss: 0.3700 - acc: 0.8708 - val_loss: 1.8426 - val_acc: 0.4838\n",
      "Epoch 41/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.3443 - acc: 0.8650 - val_loss: 2.0537 - val_acc: 0.4464\n",
      "Epoch 42/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.3910 - acc: 0.8574 - val_loss: 1.8385 - val_acc: 0.4838\n",
      "Epoch 43/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.2976 - acc: 0.8959 - val_loss: 1.9073 - val_acc: 0.4696\n",
      "Epoch 44/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.2902 - acc: 0.8939 - val_loss: 2.3323 - val_acc: 0.4727\n",
      "Epoch 45/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.2639 - acc: 0.9040 - val_loss: 2.4117 - val_acc: 0.4352\n",
      "Epoch 46/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.3612 - acc: 0.8675 - val_loss: 1.9387 - val_acc: 0.4676\n",
      "Epoch 47/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.2267 - acc: 0.9260 - val_loss: 2.0796 - val_acc: 0.4706\n",
      "Epoch 48/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.2262 - acc: 0.9177 - val_loss: 2.1723 - val_acc: 0.4757\n",
      "Epoch 49/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.2509 - acc: 0.9025 - val_loss: 2.1854 - val_acc: 0.4686\n",
      "Epoch 50/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.2297 - acc: 0.9151 - val_loss: 2.2241 - val_acc: 0.4747\n",
      "Epoch 51/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1996 - acc: 0.9288 - val_loss: 2.6384 - val_acc: 0.4747\n",
      "Epoch 52/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1800 - acc: 0.9374 - val_loss: 2.9173 - val_acc: 0.4231\n",
      "Epoch 53/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1763 - acc: 0.9357 - val_loss: 2.5557 - val_acc: 0.4352\n",
      "Epoch 54/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.2333 - acc: 0.9255 - val_loss: 2.4388 - val_acc: 0.4798\n",
      "Epoch 55/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.1417 - acc: 0.9516 - val_loss: 2.7239 - val_acc: 0.4717\n",
      "Epoch 56/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.2055 - acc: 0.9288 - val_loss: 2.3910 - val_acc: 0.4909\n",
      "Epoch 57/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.1196 - acc: 0.9605 - val_loss: 2.5077 - val_acc: 0.4879\n",
      "Epoch 58/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.1483 - acc: 0.9435 - val_loss: 2.5020 - val_acc: 0.4605\n",
      "Epoch 59/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.1651 - acc: 0.9501 - val_loss: 2.6373 - val_acc: 0.4706\n",
      "Epoch 60/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.1307 - acc: 0.9554 - val_loss: 2.5668 - val_acc: 0.4818\n",
      "Epoch 61/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0941 - acc: 0.9706 - val_loss: 2.9213 - val_acc: 0.4646\n",
      "Epoch 62/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.1086 - acc: 0.9625 - val_loss: 3.0247 - val_acc: 0.4717\n",
      "Epoch 63/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.2247 - acc: 0.9235 - val_loss: 2.3893 - val_acc: 0.4403\n",
      "Epoch 64/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.2064 - acc: 0.9453 - val_loss: 2.5394 - val_acc: 0.4372\n",
      "Epoch 65/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.0765 - acc: 0.9764 - val_loss: 2.8566 - val_acc: 0.4777\n",
      "Epoch 66/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0909 - acc: 0.9676 - val_loss: 3.0042 - val_acc: 0.4585\n",
      "Epoch 67/80\n",
      "3948/3948 [==============================] - 0s 124us/step - loss: 0.1275 - acc: 0.9547 - val_loss: 3.7187 - val_acc: 0.4109\n",
      "Epoch 68/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.2759 - acc: 0.9195 - val_loss: 2.6743 - val_acc: 0.4787\n",
      "Epoch 69/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0580 - acc: 0.9845 - val_loss: 2.7310 - val_acc: 0.4666\n",
      "Epoch 70/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.0793 - acc: 0.9734 - val_loss: 2.9231 - val_acc: 0.4646\n",
      "Epoch 71/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0548 - acc: 0.9838 - val_loss: 3.1585 - val_acc: 0.4433\n",
      "Epoch 72/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.2023 - acc: 0.9445 - val_loss: 3.4314 - val_acc: 0.4059\n",
      "Epoch 73/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.1300 - acc: 0.9673 - val_loss: 2.8307 - val_acc: 0.4615\n",
      "Epoch 74/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.0412 - acc: 0.9911 - val_loss: 3.1151 - val_acc: 0.4717\n",
      "Epoch 75/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0475 - acc: 0.9861 - val_loss: 3.0752 - val_acc: 0.4626\n",
      "Epoch 76/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0962 - acc: 0.9709 - val_loss: 3.1902 - val_acc: 0.4595\n",
      "Epoch 77/80\n",
      "3948/3948 [==============================] - 0s 126us/step - loss: 0.0506 - acc: 0.9851 - val_loss: 3.1756 - val_acc: 0.4646\n",
      "Epoch 78/80\n",
      "3948/3948 [==============================] - 1s 127us/step - loss: 0.0684 - acc: 0.9782 - val_loss: 3.1002 - val_acc: 0.4605\n",
      "Epoch 79/80\n",
      "3948/3948 [==============================] - 0s 124us/step - loss: 0.0362 - acc: 0.9894 - val_loss: 3.3579 - val_acc: 0.4757\n",
      "Epoch 80/80\n",
      "3948/3948 [==============================] - 0s 125us/step - loss: 0.0409 - acc: 0.9866 - val_loss: 3.4345 - val_acc: 0.4767\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech2, Y, \n",
    "                 batch_size=100, nb_epoch=80, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_combined(optimizer='Adadelta'):\n",
    "    modela = Sequential()\n",
    "    modela.add(Flatten(input_shape=(100, 34)))\n",
    "    modela.add(Dense(1024))\n",
    "    modela.add(Activation('relu'))\n",
    "    modela.add(Dense(512))\n",
    "    \n",
    "    modelb = Sequential()\n",
    "    modelb.add(Flatten(input_shape=(100, 34)))\n",
    "    modelb.add(Dense(1024))\n",
    "    modelb.add(Activation('relu'))\n",
    "    modelb.add(Dense(512))\n",
    "    \n",
    "    model_combined = Sequential()\n",
    "    model_combined.add(Merge([modela, modelb], mode='concat'))\n",
    "    model_combined.add(Activation('relu'))\n",
    "    model_combined.add(Dense(256))\n",
    "    model_combined.add(Activation('relu'))\n",
    "    \n",
    "    model_combined.add(Dense(4))\n",
    "    model_combined.add(Activation('softmax'))\n",
    "\n",
    "    model_combined.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 8,278,276\n",
      "Trainable params: 8,278,276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model = linear_model_combined()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/80\n",
      "3948/3948 [==============================] - 1s 340us/step - loss: 2.0201 - acc: 0.3384 - val_loss: 1.4282 - val_acc: 0.3866\n",
      "Epoch 2/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 1.2688 - acc: 0.4078 - val_loss: 1.3159 - val_acc: 0.4241\n",
      "Epoch 3/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 1.2075 - acc: 0.4491 - val_loss: 1.2337 - val_acc: 0.4190\n",
      "Epoch 4/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 1.1629 - acc: 0.4648 - val_loss: 1.3288 - val_acc: 0.4383\n",
      "Epoch 5/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 1.1358 - acc: 0.4790 - val_loss: 1.2145 - val_acc: 0.4251\n",
      "Epoch 6/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 1.0970 - acc: 0.5000 - val_loss: 1.2968 - val_acc: 0.4140\n",
      "Epoch 7/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 1.0950 - acc: 0.5048 - val_loss: 1.4335 - val_acc: 0.3917\n",
      "Epoch 8/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 1.0888 - acc: 0.5000 - val_loss: 1.2128 - val_acc: 0.4545\n",
      "Epoch 9/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 1.0370 - acc: 0.5420 - val_loss: 1.3526 - val_acc: 0.4008\n",
      "Epoch 10/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 1.0412 - acc: 0.5266 - val_loss: 1.2336 - val_acc: 0.4696\n",
      "Epoch 11/80\n",
      "3948/3948 [==============================] - 1s 224us/step - loss: 1.0087 - acc: 0.5502 - val_loss: 1.5172 - val_acc: 0.4231\n",
      "Epoch 12/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.9943 - acc: 0.5560 - val_loss: 1.4439 - val_acc: 0.3735\n",
      "Epoch 13/80\n",
      "3948/3948 [==============================] - 1s 225us/step - loss: 0.9777 - acc: 0.5727 - val_loss: 1.2553 - val_acc: 0.4899\n",
      "Epoch 14/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.9588 - acc: 0.5765 - val_loss: 1.2092 - val_acc: 0.4777\n",
      "Epoch 15/80\n",
      "3948/3948 [==============================] - 1s 225us/step - loss: 0.9436 - acc: 0.5932 - val_loss: 1.5535 - val_acc: 0.4403\n",
      "Epoch 16/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.9312 - acc: 0.5917 - val_loss: 1.3215 - val_acc: 0.4737\n",
      "Epoch 17/80\n",
      "3948/3948 [==============================] - 1s 225us/step - loss: 0.8995 - acc: 0.6092 - val_loss: 1.2050 - val_acc: 0.4848\n",
      "Epoch 18/80\n",
      "3948/3948 [==============================] - 1s 224us/step - loss: 0.8969 - acc: 0.6074 - val_loss: 1.2484 - val_acc: 0.4585\n",
      "Epoch 19/80\n",
      "3948/3948 [==============================] - 1s 225us/step - loss: 0.8697 - acc: 0.6198 - val_loss: 1.2234 - val_acc: 0.4929\n",
      "Epoch 20/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.8462 - acc: 0.6396 - val_loss: 1.2451 - val_acc: 0.5081\n",
      "Epoch 21/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.8274 - acc: 0.6426 - val_loss: 1.4167 - val_acc: 0.4555\n",
      "Epoch 22/80\n",
      "3948/3948 [==============================] - 1s 219us/step - loss: 0.7959 - acc: 0.6596 - val_loss: 1.3035 - val_acc: 0.4909\n",
      "Epoch 23/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.8071 - acc: 0.6568 - val_loss: 1.2491 - val_acc: 0.5020\n",
      "Epoch 24/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.7483 - acc: 0.6917 - val_loss: 1.3938 - val_acc: 0.4879\n",
      "Epoch 25/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.7280 - acc: 0.7014 - val_loss: 1.5590 - val_acc: 0.4403\n",
      "Epoch 26/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.7041 - acc: 0.7072 - val_loss: 1.6141 - val_acc: 0.4362\n",
      "Epoch 27/80\n",
      "3948/3948 [==============================] - 1s 219us/step - loss: 0.7206 - acc: 0.7057 - val_loss: 1.8032 - val_acc: 0.4413\n",
      "Epoch 28/80\n",
      "3948/3948 [==============================] - 1s 218us/step - loss: 0.6574 - acc: 0.7320 - val_loss: 1.3737 - val_acc: 0.4949\n",
      "Epoch 29/80\n",
      "3948/3948 [==============================] - 1s 219us/step - loss: 0.6476 - acc: 0.7249 - val_loss: 1.5796 - val_acc: 0.4868\n",
      "Epoch 30/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.6092 - acc: 0.7472 - val_loss: 1.6912 - val_acc: 0.4828\n",
      "Epoch 31/80\n",
      "3948/3948 [==============================] - 1s 218us/step - loss: 0.5893 - acc: 0.7609 - val_loss: 1.5928 - val_acc: 0.4777\n",
      "Epoch 32/80\n",
      "3948/3948 [==============================] - 1s 219us/step - loss: 0.5649 - acc: 0.7672 - val_loss: 1.4873 - val_acc: 0.4798\n",
      "Epoch 33/80\n",
      "3948/3948 [==============================] - 1s 224us/step - loss: 0.5472 - acc: 0.7844 - val_loss: 1.5491 - val_acc: 0.4696\n",
      "Epoch 34/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.5170 - acc: 0.8009 - val_loss: 1.5647 - val_acc: 0.4585\n",
      "Epoch 35/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.4779 - acc: 0.8131 - val_loss: 1.7342 - val_acc: 0.5020\n",
      "Epoch 36/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.4661 - acc: 0.8174 - val_loss: 1.7833 - val_acc: 0.4656\n",
      "Epoch 37/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.4206 - acc: 0.8369 - val_loss: 1.7092 - val_acc: 0.4747\n",
      "Epoch 38/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.4185 - acc: 0.8412 - val_loss: 1.5938 - val_acc: 0.4798\n",
      "Epoch 39/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.4005 - acc: 0.8480 - val_loss: 1.6144 - val_acc: 0.4818\n",
      "Epoch 40/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.3233 - acc: 0.8799 - val_loss: 1.7247 - val_acc: 0.4686\n",
      "Epoch 41/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.3374 - acc: 0.8693 - val_loss: 1.8922 - val_acc: 0.4686\n",
      "Epoch 42/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.3303 - acc: 0.8815 - val_loss: 1.9171 - val_acc: 0.4646\n",
      "Epoch 43/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.2873 - acc: 0.8929 - val_loss: 2.1719 - val_acc: 0.4514\n",
      "Epoch 44/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.2641 - acc: 0.9063 - val_loss: 2.2592 - val_acc: 0.4889\n",
      "Epoch 45/80\n",
      "3948/3948 [==============================] - 1s 226us/step - loss: 0.2664 - acc: 0.9020 - val_loss: 2.0102 - val_acc: 0.4858\n",
      "Epoch 46/80\n",
      "3948/3948 [==============================] - 1s 225us/step - loss: 0.2374 - acc: 0.9098 - val_loss: 2.0625 - val_acc: 0.5091\n",
      "Epoch 47/80\n",
      "3948/3948 [==============================] - 1s 224us/step - loss: 0.3591 - acc: 0.8716 - val_loss: 2.0264 - val_acc: 0.4838\n",
      "Epoch 48/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.1839 - acc: 0.9334 - val_loss: 2.1338 - val_acc: 0.4717\n",
      "Epoch 49/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.1825 - acc: 0.9357 - val_loss: 2.7800 - val_acc: 0.4484\n",
      "Epoch 50/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.2094 - acc: 0.9293 - val_loss: 2.2200 - val_acc: 0.4646\n",
      "Epoch 51/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.2097 - acc: 0.9200 - val_loss: 2.0806 - val_acc: 0.4879\n",
      "Epoch 52/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.1284 - acc: 0.9549 - val_loss: 2.2605 - val_acc: 0.4970\n",
      "Epoch 53/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.2669 - acc: 0.9043 - val_loss: 2.0664 - val_acc: 0.4929\n",
      "Epoch 54/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.1130 - acc: 0.9630 - val_loss: 2.6436 - val_acc: 0.4636\n",
      "Epoch 55/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.1126 - acc: 0.9623 - val_loss: 2.5840 - val_acc: 0.4919\n",
      "Epoch 56/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.1198 - acc: 0.9602 - val_loss: 2.8554 - val_acc: 0.4798\n",
      "Epoch 57/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.2780 - acc: 0.9207 - val_loss: 2.3913 - val_acc: 0.4787\n",
      "Epoch 58/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.0833 - acc: 0.9724 - val_loss: 2.6449 - val_acc: 0.4808\n",
      "Epoch 59/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.1039 - acc: 0.9562 - val_loss: 2.4883 - val_acc: 0.4980\n",
      "Epoch 60/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.2446 - acc: 0.9255 - val_loss: 2.5799 - val_acc: 0.4848\n",
      "Epoch 61/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.0720 - acc: 0.9724 - val_loss: 2.9502 - val_acc: 0.4281\n",
      "Epoch 62/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.1122 - acc: 0.9607 - val_loss: 2.7480 - val_acc: 0.5000\n",
      "Epoch 63/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.3093 - acc: 0.9093 - val_loss: 1.8102 - val_acc: 0.4838\n",
      "Epoch 64/80\n",
      "3948/3948 [==============================] - 1s 224us/step - loss: 0.0721 - acc: 0.9840 - val_loss: 2.5339 - val_acc: 0.5020\n",
      "Epoch 65/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.0564 - acc: 0.9818 - val_loss: 3.0100 - val_acc: 0.4879\n",
      "Epoch 66/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.0874 - acc: 0.9666 - val_loss: 2.7665 - val_acc: 0.4868\n",
      "Epoch 67/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.1034 - acc: 0.9706 - val_loss: 2.8163 - val_acc: 0.4757\n",
      "Epoch 68/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.1344 - acc: 0.9585 - val_loss: 2.6808 - val_acc: 0.4848\n",
      "Epoch 69/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.0704 - acc: 0.9732 - val_loss: 3.0227 - val_acc: 0.4403\n",
      "Epoch 70/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.0604 - acc: 0.9833 - val_loss: 2.8474 - val_acc: 0.4787\n",
      "Epoch 71/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.0448 - acc: 0.9868 - val_loss: 2.9465 - val_acc: 0.4666\n",
      "Epoch 72/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.3418 - acc: 0.9040 - val_loss: 2.3771 - val_acc: 0.4798\n",
      "Epoch 73/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.0394 - acc: 0.9921 - val_loss: 2.7843 - val_acc: 0.4818\n",
      "Epoch 74/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.0402 - acc: 0.9861 - val_loss: 2.9370 - val_acc: 0.4828\n",
      "Epoch 75/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.0661 - acc: 0.9754 - val_loss: 2.8942 - val_acc: 0.4818\n",
      "Epoch 76/80\n",
      "3948/3948 [==============================] - 1s 221us/step - loss: 0.0379 - acc: 0.9891 - val_loss: 3.1681 - val_acc: 0.4676\n",
      "Epoch 77/80\n",
      "3948/3948 [==============================] - 1s 220us/step - loss: 0.0413 - acc: 0.9873 - val_loss: 3.2264 - val_acc: 0.4848\n",
      "Epoch 78/80\n",
      "3948/3948 [==============================] - 1s 222us/step - loss: 0.0755 - acc: 0.9759 - val_loss: 3.0819 - val_acc: 0.4747\n",
      "Epoch 79/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.0202 - acc: 0.9944 - val_loss: 3.2629 - val_acc: 0.4929\n",
      "Epoch 80/80\n",
      "3948/3948 [==============================] - 1s 223us/step - loss: 0.1945 - acc: 0.9448 - val_loss: 2.9328 - val_acc: 0.3877\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([x_train_speech, x_train_speech2], Y, \n",
    "                 batch_size=100, nb_epoch=80, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent, _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(100, 34)))\n",
    "    model.add(AttentionDecoder(128,128))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100, 128)          83456     \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 100, 128)          246528    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 6,886,148\n",
      "Trainable params: 6,886,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = attention_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/60\n",
      "3948/3948 [==============================] - 25s - loss: 1.3642 - acc: 0.3384 - val_loss: 1.3344 - val_acc: 0.3917\n",
      "Epoch 2/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.3507 - acc: 0.3495 - val_loss: 1.3477 - val_acc: 0.3704\n",
      "Epoch 3/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.3370 - acc: 0.3544 - val_loss: 1.3278 - val_acc: 0.3745\n",
      "Epoch 4/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.2908 - acc: 0.3549 - val_loss: 1.2944 - val_acc: 0.4221\n",
      "Epoch 5/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.2461 - acc: 0.4015 - val_loss: 1.3509 - val_acc: 0.2298\n",
      "Epoch 6/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.2416 - acc: 0.4098 - val_loss: 1.2243 - val_acc: 0.4636\n",
      "Epoch 7/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1967 - acc: 0.4271 - val_loss: 1.2296 - val_acc: 0.4261\n",
      "Epoch 8/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1825 - acc: 0.4488 - val_loss: 1.2251 - val_acc: 0.3887\n",
      "Epoch 9/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1851 - acc: 0.4450 - val_loss: 1.1586 - val_acc: 0.5000\n",
      "Epoch 10/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1550 - acc: 0.4618 - val_loss: 1.3708 - val_acc: 0.3451\n",
      "Epoch 11/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1515 - acc: 0.4643 - val_loss: 1.1823 - val_acc: 0.4919\n",
      "Epoch 12/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1328 - acc: 0.4744 - val_loss: 1.1906 - val_acc: 0.4524\n",
      "Epoch 13/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1362 - acc: 0.4754 - val_loss: 1.2039 - val_acc: 0.4453\n",
      "Epoch 14/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1238 - acc: 0.4754 - val_loss: 1.1405 - val_acc: 0.5040\n",
      "Epoch 15/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1203 - acc: 0.4782 - val_loss: 1.1553 - val_acc: 0.4889\n",
      "Epoch 16/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1064 - acc: 0.4866 - val_loss: 1.3970 - val_acc: 0.3360\n",
      "Epoch 17/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.1033 - acc: 0.4977 - val_loss: 1.1755 - val_acc: 0.4322\n",
      "Epoch 18/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0946 - acc: 0.4972 - val_loss: 1.1274 - val_acc: 0.4767\n",
      "Epoch 19/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0798 - acc: 0.5008 - val_loss: 1.1156 - val_acc: 0.4858\n",
      "Epoch 20/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0728 - acc: 0.5046 - val_loss: 1.1252 - val_acc: 0.4636\n",
      "Epoch 21/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0824 - acc: 0.5051 - val_loss: 1.1091 - val_acc: 0.5030\n",
      "Epoch 22/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0723 - acc: 0.5010 - val_loss: 1.1598 - val_acc: 0.5091\n",
      "Epoch 23/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0611 - acc: 0.5104 - val_loss: 1.1126 - val_acc: 0.5142\n",
      "Epoch 24/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0419 - acc: 0.5218 - val_loss: 1.4665 - val_acc: 0.3472\n",
      "Epoch 25/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0432 - acc: 0.5274 - val_loss: 1.1310 - val_acc: 0.4960\n",
      "Epoch 26/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0326 - acc: 0.5286 - val_loss: 1.1554 - val_acc: 0.4484\n",
      "Epoch 27/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0295 - acc: 0.5327 - val_loss: 1.3083 - val_acc: 0.4038\n",
      "Epoch 28/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0174 - acc: 0.5370 - val_loss: 1.1428 - val_acc: 0.4767\n",
      "Epoch 29/60\n",
      "3948/3948 [==============================] - 24s - loss: 1.0079 - acc: 0.5502 - val_loss: 1.1470 - val_acc: 0.4838\n",
      "Epoch 30/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9965 - acc: 0.5565 - val_loss: 1.1854 - val_acc: 0.4686\n",
      "Epoch 31/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9876 - acc: 0.5661 - val_loss: 1.0918 - val_acc: 0.5304\n",
      "Epoch 32/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9800 - acc: 0.5638 - val_loss: 1.0934 - val_acc: 0.5121\n",
      "Epoch 33/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9692 - acc: 0.5773 - val_loss: 1.2146 - val_acc: 0.4777\n",
      "Epoch 34/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9584 - acc: 0.5729 - val_loss: 1.1351 - val_acc: 0.5314\n",
      "Epoch 35/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9532 - acc: 0.5745 - val_loss: 1.1323 - val_acc: 0.5162\n",
      "Epoch 36/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9551 - acc: 0.5846 - val_loss: 1.2285 - val_acc: 0.4858\n",
      "Epoch 37/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9439 - acc: 0.5849 - val_loss: 1.1086 - val_acc: 0.5162\n",
      "Epoch 38/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9345 - acc: 0.5831 - val_loss: 1.1140 - val_acc: 0.5202\n",
      "Epoch 39/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9261 - acc: 0.5881 - val_loss: 1.2098 - val_acc: 0.4889\n",
      "Epoch 40/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9140 - acc: 0.5957 - val_loss: 1.1297 - val_acc: 0.5182\n",
      "Epoch 41/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9129 - acc: 0.6018 - val_loss: 1.1575 - val_acc: 0.5111\n",
      "Epoch 42/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9177 - acc: 0.5919 - val_loss: 1.1657 - val_acc: 0.5213\n",
      "Epoch 43/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9018 - acc: 0.6051 - val_loss: 1.2202 - val_acc: 0.5233\n",
      "Epoch 44/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.9008 - acc: 0.5960 - val_loss: 1.0912 - val_acc: 0.5253\n",
      "Epoch 45/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8849 - acc: 0.6028 - val_loss: 1.0739 - val_acc: 0.5385\n",
      "Epoch 46/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8856 - acc: 0.6109 - val_loss: 1.1355 - val_acc: 0.5121\n",
      "Epoch 47/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8765 - acc: 0.6125 - val_loss: 1.0879 - val_acc: 0.5314\n",
      "Epoch 48/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8643 - acc: 0.6231 - val_loss: 1.2044 - val_acc: 0.5111\n",
      "Epoch 49/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8570 - acc: 0.6292 - val_loss: 1.2759 - val_acc: 0.4939\n",
      "Epoch 50/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8602 - acc: 0.6282 - val_loss: 1.1732 - val_acc: 0.5101\n",
      "Epoch 51/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8460 - acc: 0.6264 - val_loss: 1.1997 - val_acc: 0.5111\n",
      "Epoch 52/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8474 - acc: 0.6269 - val_loss: 1.1597 - val_acc: 0.5233\n",
      "Epoch 53/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8343 - acc: 0.6373 - val_loss: 1.1355 - val_acc: 0.5121\n",
      "Epoch 54/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8327 - acc: 0.6355 - val_loss: 1.2845 - val_acc: 0.5061\n",
      "Epoch 55/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8228 - acc: 0.6439 - val_loss: 1.2401 - val_acc: 0.5071\n",
      "Epoch 56/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8087 - acc: 0.6505 - val_loss: 1.2088 - val_acc: 0.5192\n",
      "Epoch 57/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.8152 - acc: 0.6454 - val_loss: 1.1145 - val_acc: 0.5415\n",
      "Epoch 58/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.7962 - acc: 0.6575 - val_loss: 1.3575 - val_acc: 0.5202\n",
      "Epoch 59/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.7890 - acc: 0.6568 - val_loss: 1.2627 - val_acc: 0.5162\n",
      "Epoch 60/60\n",
      "3948/3948 [==============================] - 24s - loss: 0.7808 - acc: 0.6667 - val_loss: 1.2680 - val_acc: 0.5192\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=60, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_3(frames, freq, options):\n",
    "    #double the window duration\n",
    "    window_sec = 0.08\n",
    "    window_n = int(freq * window_sec)\n",
    "\n",
    "    st_f = stFeatureExtraction(frames, freq, window_n, window_n / 2)\n",
    "\n",
    "    if st_f.shape[1] > 2:\n",
    "        i0 = 1\n",
    "        i1 = st_f.shape[1] - 1\n",
    "        if i1 - i0 < 1:\n",
    "            i1 = i0 + 1\n",
    "        \n",
    "        deriv_st_f = np.zeros((st_f.shape[0], i1 - i0), dtype=float)\n",
    "        for i in range(i0, i1):\n",
    "            i_left = i - 1\n",
    "            i_right = i + 1\n",
    "            deriv_st_f[:st_f.shape[0], i - i0] = st_f[:, i]\n",
    "        return deriv_st_f\n",
    "    elif st_f.shape[1] == 2:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f\n",
    "    else:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4936, 200, 34)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_speech3 = []\n",
    "from sklearn.preprocessing import normalize\n",
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    x_head = ses_mod['signal']\n",
    "    st_features = calculate_features_3(x_head, framerate, None)\n",
    "    st_features, _ = pad_sequence_into_array(st_features, maxlen=200)\n",
    "    x_train_speech3.append( st_features.T )\n",
    "    counter+=1\n",
    "    if(counter%100==0):\n",
    "        print(counter)\n",
    "    \n",
    "x_train_speech3 = np.array(x_train_speech3)\n",
    "x_train_speech3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model2(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(200, 34)))\n",
    "    model.add(AttentionDecoder(128,128))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200, 128)          83456     \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 200, 128)          246528    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 13,439,748\n",
      "Trainable params: 13,439,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = attention_model2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/75\n",
      "3948/3948 [==============================] - 62s - loss: 1.3628 - acc: 0.3399 - val_loss: 1.3550 - val_acc: 0.3846\n",
      "Epoch 2/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.3551 - acc: 0.3612 - val_loss: 1.3565 - val_acc: 0.3856\n",
      "Epoch 3/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.3522 - acc: 0.3597 - val_loss: 1.3615 - val_acc: 0.3836\n",
      "Epoch 4/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.3517 - acc: 0.3604 - val_loss: 1.3573 - val_acc: 0.3917\n",
      "Epoch 5/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.3518 - acc: 0.3582 - val_loss: 1.3626 - val_acc: 0.3725\n",
      "Epoch 6/75\n",
      "3948/3948 [==============================] - 60s - loss: 1.3404 - acc: 0.3582 - val_loss: 1.3488 - val_acc: 0.2885\n",
      "Epoch 7/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.3124 - acc: 0.4007 - val_loss: 1.4772 - val_acc: 0.2206\n",
      "Epoch 8/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.2383 - acc: 0.4298 - val_loss: 1.2223 - val_acc: 0.3866\n",
      "Epoch 9/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1984 - acc: 0.4443 - val_loss: 1.2333 - val_acc: 0.4130\n",
      "Epoch 10/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1804 - acc: 0.4536 - val_loss: 1.2290 - val_acc: 0.4545\n",
      "Epoch 11/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1493 - acc: 0.4805 - val_loss: 1.1527 - val_acc: 0.4757\n",
      "Epoch 12/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1439 - acc: 0.4802 - val_loss: 1.2542 - val_acc: 0.3462\n",
      "Epoch 13/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1338 - acc: 0.4732 - val_loss: 1.1683 - val_acc: 0.5121\n",
      "Epoch 14/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1078 - acc: 0.4853 - val_loss: 1.1465 - val_acc: 0.4494\n",
      "Epoch 15/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1096 - acc: 0.4878 - val_loss: 1.1651 - val_acc: 0.5061\n",
      "Epoch 16/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.1034 - acc: 0.4868 - val_loss: 1.1409 - val_acc: 0.5142\n",
      "Epoch 17/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0818 - acc: 0.5000 - val_loss: 1.1636 - val_acc: 0.4372\n",
      "Epoch 18/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0894 - acc: 0.4947 - val_loss: 1.1752 - val_acc: 0.4423\n",
      "Epoch 19/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0811 - acc: 0.5079 - val_loss: 1.1154 - val_acc: 0.5061\n",
      "Epoch 20/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0757 - acc: 0.5010 - val_loss: 1.0943 - val_acc: 0.5192\n",
      "Epoch 21/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0633 - acc: 0.5051 - val_loss: 1.1162 - val_acc: 0.4960\n",
      "Epoch 22/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0613 - acc: 0.5038 - val_loss: 1.1112 - val_acc: 0.4949\n",
      "Epoch 23/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0534 - acc: 0.5114 - val_loss: 1.1956 - val_acc: 0.4879\n",
      "Epoch 24/75\n",
      "3948/3948 [==============================] - 62s - loss: 1.0479 - acc: 0.5048 - val_loss: 1.0952 - val_acc: 0.5273\n",
      "Epoch 25/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0294 - acc: 0.5251 - val_loss: 1.1024 - val_acc: 0.5314\n",
      "Epoch 26/75\n",
      "3948/3948 [==============================] - 62s - loss: 1.0409 - acc: 0.5109 - val_loss: 1.1581 - val_acc: 0.4960\n",
      "Epoch 27/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0296 - acc: 0.5155 - val_loss: 1.1493 - val_acc: 0.4909\n",
      "Epoch 28/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0304 - acc: 0.5149 - val_loss: 1.0989 - val_acc: 0.5030\n",
      "Epoch 29/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0191 - acc: 0.5261 - val_loss: 1.0997 - val_acc: 0.5192\n",
      "Epoch 30/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0114 - acc: 0.5301 - val_loss: 1.1587 - val_acc: 0.4615\n",
      "Epoch 31/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0178 - acc: 0.5271 - val_loss: 1.1004 - val_acc: 0.5020\n",
      "Epoch 32/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0188 - acc: 0.5286 - val_loss: 1.1262 - val_acc: 0.5111\n",
      "Epoch 33/75\n",
      "3948/3948 [==============================] - 61s - loss: 1.0090 - acc: 0.5276 - val_loss: 1.0781 - val_acc: 0.4939\n",
      "Epoch 34/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9963 - acc: 0.5370 - val_loss: 1.0935 - val_acc: 0.5273\n",
      "Epoch 35/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9886 - acc: 0.5438 - val_loss: 1.0834 - val_acc: 0.5162\n",
      "Epoch 36/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9891 - acc: 0.5423 - val_loss: 1.1088 - val_acc: 0.5223\n",
      "Epoch 37/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9842 - acc: 0.5428 - val_loss: 1.1258 - val_acc: 0.4808\n",
      "Epoch 38/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9778 - acc: 0.5453 - val_loss: 1.1639 - val_acc: 0.4939\n",
      "Epoch 39/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9739 - acc: 0.5491 - val_loss: 1.1190 - val_acc: 0.4696\n",
      "Epoch 40/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9655 - acc: 0.5540 - val_loss: 1.1217 - val_acc: 0.5324\n",
      "Epoch 41/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9508 - acc: 0.5752 - val_loss: 1.1191 - val_acc: 0.5202\n",
      "Epoch 42/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9515 - acc: 0.5732 - val_loss: 1.1246 - val_acc: 0.5132\n",
      "Epoch 43/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9386 - acc: 0.5775 - val_loss: 1.1430 - val_acc: 0.5172\n",
      "Epoch 44/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9240 - acc: 0.5876 - val_loss: 1.1423 - val_acc: 0.5081\n",
      "Epoch 45/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9304 - acc: 0.5833 - val_loss: 1.1123 - val_acc: 0.5213\n",
      "Epoch 46/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9173 - acc: 0.5925 - val_loss: 1.1677 - val_acc: 0.5344\n",
      "Epoch 47/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.9121 - acc: 0.5935 - val_loss: 1.0900 - val_acc: 0.5283\n",
      "Epoch 48/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8994 - acc: 0.6041 - val_loss: 1.0860 - val_acc: 0.5162\n",
      "Epoch 49/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8895 - acc: 0.6018 - val_loss: 1.1514 - val_acc: 0.4980\n",
      "Epoch 50/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8845 - acc: 0.6097 - val_loss: 1.1666 - val_acc: 0.4939\n",
      "Epoch 51/75\n",
      "3948/3948 [==============================] - 62s - loss: 0.8771 - acc: 0.6173 - val_loss: 1.1152 - val_acc: 0.5344\n",
      "Epoch 52/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8704 - acc: 0.6165 - val_loss: 1.0736 - val_acc: 0.5445\n",
      "Epoch 53/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8654 - acc: 0.6185 - val_loss: 1.1149 - val_acc: 0.5040\n",
      "Epoch 54/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8495 - acc: 0.6297 - val_loss: 1.1376 - val_acc: 0.5294\n",
      "Epoch 55/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8322 - acc: 0.6403 - val_loss: 1.1893 - val_acc: 0.5091\n",
      "Epoch 56/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8263 - acc: 0.6403 - val_loss: 1.2883 - val_acc: 0.4889\n",
      "Epoch 57/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8261 - acc: 0.6368 - val_loss: 1.1498 - val_acc: 0.5152\n",
      "Epoch 58/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8217 - acc: 0.6512 - val_loss: 1.1983 - val_acc: 0.4737\n",
      "Epoch 59/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.8116 - acc: 0.6464 - val_loss: 1.1247 - val_acc: 0.5374\n",
      "Epoch 60/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7955 - acc: 0.6611 - val_loss: 1.2462 - val_acc: 0.4787\n",
      "Epoch 61/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7928 - acc: 0.6613 - val_loss: 1.2109 - val_acc: 0.5152\n",
      "Epoch 62/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7751 - acc: 0.6738 - val_loss: 1.1824 - val_acc: 0.5202\n",
      "Epoch 63/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7739 - acc: 0.6684 - val_loss: 1.3074 - val_acc: 0.5051\n",
      "Epoch 64/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7540 - acc: 0.6816 - val_loss: 1.2076 - val_acc: 0.5162\n",
      "Epoch 65/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7607 - acc: 0.6791 - val_loss: 1.2630 - val_acc: 0.5101\n",
      "Epoch 66/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7318 - acc: 0.6892 - val_loss: 1.2338 - val_acc: 0.5263\n",
      "Epoch 67/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7514 - acc: 0.6841 - val_loss: 1.1518 - val_acc: 0.5213\n",
      "Epoch 68/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7306 - acc: 0.6925 - val_loss: 1.1846 - val_acc: 0.5395\n",
      "Epoch 69/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7245 - acc: 0.6933 - val_loss: 1.2834 - val_acc: 0.5081\n",
      "Epoch 70/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.7176 - acc: 0.7009 - val_loss: 1.3756 - val_acc: 0.5071\n",
      "Epoch 71/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.6963 - acc: 0.7158 - val_loss: 1.4488 - val_acc: 0.4929\n",
      "Epoch 72/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.6801 - acc: 0.7209 - val_loss: 1.2931 - val_acc: 0.4889\n",
      "Epoch 73/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.6725 - acc: 0.7188 - val_loss: 1.3620 - val_acc: 0.5202\n",
      "Epoch 74/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.6647 - acc: 0.7219 - val_loss: 1.2873 - val_acc: 0.5182\n",
      "Epoch 75/75\n",
      "3948/3948 [==============================] - 61s - loss: 0.6532 - acc: 0.7239 - val_loss: 1.3066 - val_acc: 0.5132\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech3, Y, \n",
    "                 batch_size=100, nb_epoch=75, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3838"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    if (ses_mod['id'][:5]==\"Ses05\"):\n",
    "        break\n",
    "    counter+=1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_sp = x_train_speech[:3838]\n",
    "xtest_sp = x_train_speech[3838:]\n",
    "ytrain_sp = Y[:3838]\n",
    "ytest_sp = Y[3838:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100, 128)          83456     \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 100, 128)          246528    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 6,886,148\n",
      "Trainable params: 6,886,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def attention_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(100, 34)))\n",
    "    model.add(AttentionDecoder(128,128))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = attention_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3838 samples, validate on 1098 samples\n",
      "Epoch 1/80\n",
      "3838/3838 [==============================] - 25s - loss: 1.3625 - acc: 0.3434 - val_loss: 1.3571 - val_acc: 0.3497\n",
      "Epoch 2/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.3441 - acc: 0.3473 - val_loss: 1.3971 - val_acc: 0.2514\n",
      "Epoch 3/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.3168 - acc: 0.3781 - val_loss: 1.2859 - val_acc: 0.3816\n",
      "Epoch 4/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.2488 - acc: 0.4039 - val_loss: 1.3012 - val_acc: 0.3761\n",
      "Epoch 5/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.2164 - acc: 0.4299 - val_loss: 1.2414 - val_acc: 0.3698\n",
      "Epoch 6/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1683 - acc: 0.4567 - val_loss: 1.2050 - val_acc: 0.4454\n",
      "Epoch 7/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1574 - acc: 0.4664 - val_loss: 1.2926 - val_acc: 0.4171\n",
      "Epoch 8/80\n",
      "3838/3838 [==============================] - 23s - loss: 1.1442 - acc: 0.4726 - val_loss: 1.2177 - val_acc: 0.4454\n",
      "Epoch 9/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1422 - acc: 0.4664 - val_loss: 1.1842 - val_acc: 0.4454\n",
      "Epoch 10/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1283 - acc: 0.4823 - val_loss: 1.1827 - val_acc: 0.4663\n",
      "Epoch 11/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1139 - acc: 0.4812 - val_loss: 1.2036 - val_acc: 0.4745\n",
      "Epoch 12/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1103 - acc: 0.4880 - val_loss: 1.1976 - val_acc: 0.4317\n",
      "Epoch 13/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.1016 - acc: 0.4943 - val_loss: 1.2404 - val_acc: 0.3989\n",
      "Epoch 14/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0985 - acc: 0.4914 - val_loss: 1.3050 - val_acc: 0.3871\n",
      "Epoch 15/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0856 - acc: 0.4932 - val_loss: 1.1939 - val_acc: 0.4226\n",
      "Epoch 16/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0846 - acc: 0.4935 - val_loss: 1.2063 - val_acc: 0.4499\n",
      "Epoch 17/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0830 - acc: 0.4990 - val_loss: 1.1426 - val_acc: 0.4754\n",
      "Epoch 18/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0723 - acc: 0.5078 - val_loss: 1.1736 - val_acc: 0.4627\n",
      "Epoch 19/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0684 - acc: 0.5008 - val_loss: 1.1408 - val_acc: 0.4709\n",
      "Epoch 20/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0639 - acc: 0.5099 - val_loss: 1.2740 - val_acc: 0.3816\n",
      "Epoch 21/80\n",
      "3838/3838 [==============================] - 23s - loss: 1.0603 - acc: 0.5031 - val_loss: 1.1392 - val_acc: 0.4745\n",
      "Epoch 22/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0562 - acc: 0.5164 - val_loss: 1.1435 - val_acc: 0.4927\n",
      "Epoch 23/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0517 - acc: 0.5135 - val_loss: 1.1604 - val_acc: 0.4772\n",
      "Epoch 24/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0473 - acc: 0.5130 - val_loss: 1.1771 - val_acc: 0.4872\n",
      "Epoch 25/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0426 - acc: 0.5133 - val_loss: 1.2336 - val_acc: 0.4107\n",
      "Epoch 26/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0430 - acc: 0.5125 - val_loss: 1.2549 - val_acc: 0.4244\n",
      "Epoch 27/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0387 - acc: 0.5208 - val_loss: 1.1902 - val_acc: 0.4718\n",
      "Epoch 28/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0322 - acc: 0.5206 - val_loss: 1.1811 - val_acc: 0.4709\n",
      "Epoch 29/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0251 - acc: 0.5294 - val_loss: 1.1722 - val_acc: 0.4690\n",
      "Epoch 30/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0211 - acc: 0.5373 - val_loss: 1.1632 - val_acc: 0.4672\n",
      "Epoch 31/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0110 - acc: 0.5373 - val_loss: 1.1738 - val_acc: 0.4818\n",
      "Epoch 32/80\n",
      "3838/3838 [==============================] - 24s - loss: 1.0059 - acc: 0.5399 - val_loss: 1.1618 - val_acc: 0.4909\n",
      "Epoch 33/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9973 - acc: 0.5388 - val_loss: 1.1692 - val_acc: 0.4581\n",
      "Epoch 34/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9968 - acc: 0.5469 - val_loss: 1.2998 - val_acc: 0.4590\n",
      "Epoch 35/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9904 - acc: 0.5508 - val_loss: 1.1781 - val_acc: 0.4809\n",
      "Epoch 36/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9833 - acc: 0.5539 - val_loss: 1.2439 - val_acc: 0.4499\n",
      "Epoch 37/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9747 - acc: 0.5615 - val_loss: 1.1539 - val_acc: 0.5100\n",
      "Epoch 38/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9741 - acc: 0.5615 - val_loss: 1.1803 - val_acc: 0.5118\n",
      "Epoch 39/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9678 - acc: 0.5584 - val_loss: 1.1488 - val_acc: 0.5164\n",
      "Epoch 40/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9564 - acc: 0.5685 - val_loss: 1.1847 - val_acc: 0.5146\n",
      "Epoch 41/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.9512 - acc: 0.5690 - val_loss: 1.1588 - val_acc: 0.5319\n",
      "Epoch 42/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9438 - acc: 0.5766 - val_loss: 1.1436 - val_acc: 0.5118\n",
      "Epoch 43/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9399 - acc: 0.5753 - val_loss: 1.1450 - val_acc: 0.5237\n",
      "Epoch 44/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9354 - acc: 0.5810 - val_loss: 1.2298 - val_acc: 0.5209\n",
      "Epoch 45/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9407 - acc: 0.5818 - val_loss: 1.1865 - val_acc: 0.4954\n",
      "Epoch 46/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9248 - acc: 0.5881 - val_loss: 1.1590 - val_acc: 0.5055\n",
      "Epoch 47/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.9210 - acc: 0.5852 - val_loss: 1.1743 - val_acc: 0.4991\n",
      "Epoch 48/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.9106 - acc: 0.6027 - val_loss: 1.1859 - val_acc: 0.5073\n",
      "Epoch 49/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.9050 - acc: 0.5974 - val_loss: 1.2184 - val_acc: 0.4936\n",
      "Epoch 50/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.9058 - acc: 0.6073 - val_loss: 1.2391 - val_acc: 0.5082\n",
      "Epoch 51/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8885 - acc: 0.6102 - val_loss: 1.2065 - val_acc: 0.4927\n",
      "Epoch 52/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8907 - acc: 0.6120 - val_loss: 1.1721 - val_acc: 0.5264\n",
      "Epoch 53/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8797 - acc: 0.6149 - val_loss: 1.2904 - val_acc: 0.4927\n",
      "Epoch 54/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8826 - acc: 0.6144 - val_loss: 1.2355 - val_acc: 0.5000\n",
      "Epoch 55/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8652 - acc: 0.6214 - val_loss: 1.2374 - val_acc: 0.5146\n",
      "Epoch 56/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8688 - acc: 0.6225 - val_loss: 1.1708 - val_acc: 0.5401\n",
      "Epoch 57/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8571 - acc: 0.6334 - val_loss: 1.2011 - val_acc: 0.5036\n",
      "Epoch 58/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8529 - acc: 0.6183 - val_loss: 1.2494 - val_acc: 0.4945\n",
      "Epoch 59/80\n",
      "3838/3838 [==============================] - 24s - loss: 0.8423 - acc: 0.6399 - val_loss: 1.2551 - val_acc: 0.5055\n",
      "Epoch 60/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8429 - acc: 0.6269 - val_loss: 1.1801 - val_acc: 0.5337\n",
      "Epoch 61/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8320 - acc: 0.6394 - val_loss: 1.3253 - val_acc: 0.4945\n",
      "Epoch 62/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8343 - acc: 0.6373 - val_loss: 1.2556 - val_acc: 0.5118\n",
      "Epoch 63/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8205 - acc: 0.6462 - val_loss: 1.2594 - val_acc: 0.5036\n",
      "Epoch 64/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8115 - acc: 0.6488 - val_loss: 1.3313 - val_acc: 0.4927\n",
      "Epoch 65/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.8079 - acc: 0.6511 - val_loss: 1.2606 - val_acc: 0.5328\n",
      "Epoch 66/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7899 - acc: 0.6516 - val_loss: 1.3049 - val_acc: 0.5064\n",
      "Epoch 67/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7956 - acc: 0.6519 - val_loss: 1.2126 - val_acc: 0.5464\n",
      "Epoch 68/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7868 - acc: 0.6647 - val_loss: 1.3335 - val_acc: 0.4845\n",
      "Epoch 69/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7767 - acc: 0.6631 - val_loss: 1.2787 - val_acc: 0.5027\n",
      "Epoch 70/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7766 - acc: 0.6649 - val_loss: 1.2536 - val_acc: 0.5301\n",
      "Epoch 71/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7693 - acc: 0.6688 - val_loss: 1.2805 - val_acc: 0.5073\n",
      "Epoch 72/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7547 - acc: 0.6740 - val_loss: 1.3008 - val_acc: 0.5027\n",
      "Epoch 73/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7400 - acc: 0.6897 - val_loss: 1.3795 - val_acc: 0.4854\n",
      "Epoch 74/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7535 - acc: 0.6837 - val_loss: 1.3301 - val_acc: 0.5064\n",
      "Epoch 75/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7250 - acc: 0.6970 - val_loss: 1.3156 - val_acc: 0.5082\n",
      "Epoch 76/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7324 - acc: 0.6761 - val_loss: 1.3690 - val_acc: 0.5246\n",
      "Epoch 77/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7200 - acc: 0.6939 - val_loss: 1.3501 - val_acc: 0.5109\n",
      "Epoch 78/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7153 - acc: 0.6931 - val_loss: 1.3394 - val_acc: 0.5027\n",
      "Epoch 79/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.7036 - acc: 0.6965 - val_loss: 1.5133 - val_acc: 0.4763\n",
      "Epoch 80/80\n",
      "3838/3838 [==============================] - 23s - loss: 0.6951 - acc: 0.7074 - val_loss: 1.3125 - val_acc: 0.5155\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(xtrain_sp, ytrain_sp, \n",
    "                 batch_size=100, nb_epoch=80, verbose=1, shuffle = True, \n",
    "                 validation_data=(xtest_sp, ytest_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 100, 256)          166912    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 256)          689664    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 13,966,340\n",
      "Trainable params: 13,966,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def attention_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(100, 34)))\n",
    "    model.add(Bidirectional(AttentionDecoder(128,128)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = attention_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3838 samples, validate on 1098 samples\n",
      "Epoch 1/100\n",
      "3838/3838 [==============================] - 52s - loss: 1.3633 - acc: 0.3421 - val_loss: 1.3600 - val_acc: 0.3497\n",
      "Epoch 2/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.3530 - acc: 0.3476 - val_loss: 1.3475 - val_acc: 0.3770\n",
      "Epoch 3/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.3347 - acc: 0.3502 - val_loss: 1.4572 - val_acc: 0.2796\n",
      "Epoch 4/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.3025 - acc: 0.3890 - val_loss: 1.3198 - val_acc: 0.3661\n",
      "Epoch 5/100\n",
      "3838/3838 [==============================] - 51s - loss: 1.2464 - acc: 0.4226 - val_loss: 1.2358 - val_acc: 0.4080\n",
      "Epoch 6/100\n",
      "3838/3838 [==============================] - 51s - loss: 1.1896 - acc: 0.4471 - val_loss: 1.5733 - val_acc: 0.3224\n",
      "Epoch 7/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.1769 - acc: 0.4567 - val_loss: 1.1983 - val_acc: 0.4262\n",
      "Epoch 8/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.1445 - acc: 0.4797 - val_loss: 1.1891 - val_acc: 0.4217\n",
      "Epoch 9/100\n",
      "3838/3838 [==============================] - 51s - loss: 1.1306 - acc: 0.4760 - val_loss: 1.4083 - val_acc: 0.3470\n",
      "Epoch 10/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.1016 - acc: 0.4922 - val_loss: 1.1641 - val_acc: 0.4590\n",
      "Epoch 11/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.1148 - acc: 0.4812 - val_loss: 1.2268 - val_acc: 0.4180\n",
      "Epoch 12/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.1072 - acc: 0.4865 - val_loss: 1.2099 - val_acc: 0.4390\n",
      "Epoch 13/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0903 - acc: 0.4865 - val_loss: 1.1607 - val_acc: 0.4709\n",
      "Epoch 14/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0880 - acc: 0.4906 - val_loss: 1.1926 - val_acc: 0.4472\n",
      "Epoch 15/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0744 - acc: 0.4992 - val_loss: 1.1578 - val_acc: 0.4872\n",
      "Epoch 16/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0632 - acc: 0.5039 - val_loss: 1.1496 - val_acc: 0.4872\n",
      "Epoch 17/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0590 - acc: 0.5013 - val_loss: 1.3164 - val_acc: 0.3871\n",
      "Epoch 18/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0502 - acc: 0.5146 - val_loss: 1.2005 - val_acc: 0.4463\n",
      "Epoch 19/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0543 - acc: 0.5117 - val_loss: 1.2389 - val_acc: 0.4381\n",
      "Epoch 20/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0458 - acc: 0.5052 - val_loss: 1.1594 - val_acc: 0.5064\n",
      "Epoch 21/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0371 - acc: 0.5143 - val_loss: 1.1356 - val_acc: 0.4882\n",
      "Epoch 22/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0337 - acc: 0.5208 - val_loss: 1.2472 - val_acc: 0.4208\n",
      "Epoch 23/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0355 - acc: 0.5206 - val_loss: 1.1598 - val_acc: 0.4973\n",
      "Epoch 24/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0270 - acc: 0.5156 - val_loss: 1.1444 - val_acc: 0.4982\n",
      "Epoch 25/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0143 - acc: 0.5253 - val_loss: 1.1645 - val_acc: 0.4727\n",
      "Epoch 26/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0154 - acc: 0.5370 - val_loss: 1.1361 - val_acc: 0.4964\n",
      "Epoch 27/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0080 - acc: 0.5315 - val_loss: 1.2472 - val_acc: 0.4936\n",
      "Epoch 28/100\n",
      "3838/3838 [==============================] - 50s - loss: 1.0035 - acc: 0.5347 - val_loss: 1.2413 - val_acc: 0.4791\n",
      "Epoch 29/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9926 - acc: 0.5427 - val_loss: 1.1335 - val_acc: 0.5209\n",
      "Epoch 30/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9833 - acc: 0.5500 - val_loss: 1.2514 - val_acc: 0.4399\n",
      "Epoch 31/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9775 - acc: 0.5550 - val_loss: 1.1226 - val_acc: 0.5237\n",
      "Epoch 32/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9732 - acc: 0.5539 - val_loss: 1.1459 - val_acc: 0.4973\n",
      "Epoch 33/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9595 - acc: 0.5670 - val_loss: 1.1829 - val_acc: 0.5027\n",
      "Epoch 34/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9530 - acc: 0.5701 - val_loss: 1.3752 - val_acc: 0.4098\n",
      "Epoch 35/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9557 - acc: 0.5711 - val_loss: 1.1240 - val_acc: 0.5291\n",
      "Epoch 36/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9473 - acc: 0.5696 - val_loss: 1.2047 - val_acc: 0.5027\n",
      "Epoch 37/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9263 - acc: 0.5862 - val_loss: 1.1373 - val_acc: 0.4900\n",
      "Epoch 38/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9241 - acc: 0.5857 - val_loss: 1.2555 - val_acc: 0.4845\n",
      "Epoch 39/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9164 - acc: 0.5868 - val_loss: 1.1767 - val_acc: 0.4936\n",
      "Epoch 40/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.9118 - acc: 0.5907 - val_loss: 1.1525 - val_acc: 0.5219\n",
      "Epoch 41/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8970 - acc: 0.5928 - val_loss: 1.2597 - val_acc: 0.4454\n",
      "Epoch 42/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8901 - acc: 0.5977 - val_loss: 1.2738 - val_acc: 0.4791\n",
      "Epoch 43/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8958 - acc: 0.5941 - val_loss: 1.2600 - val_acc: 0.4745\n",
      "Epoch 44/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8694 - acc: 0.6126 - val_loss: 1.4745 - val_acc: 0.4526\n",
      "Epoch 45/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8748 - acc: 0.6110 - val_loss: 1.2478 - val_acc: 0.4854\n",
      "Epoch 46/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8632 - acc: 0.6191 - val_loss: 1.1855 - val_acc: 0.4891\n",
      "Epoch 47/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8454 - acc: 0.6264 - val_loss: 1.2092 - val_acc: 0.5091\n",
      "Epoch 48/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8365 - acc: 0.6318 - val_loss: 1.2569 - val_acc: 0.4654\n",
      "Epoch 49/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8402 - acc: 0.6199 - val_loss: 1.2223 - val_acc: 0.4936\n",
      "Epoch 50/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8302 - acc: 0.6352 - val_loss: 1.1673 - val_acc: 0.4982\n",
      "Epoch 51/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8269 - acc: 0.6334 - val_loss: 1.3636 - val_acc: 0.4763\n",
      "Epoch 52/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8065 - acc: 0.6394 - val_loss: 1.3116 - val_acc: 0.4845\n",
      "Epoch 53/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8105 - acc: 0.6438 - val_loss: 1.2298 - val_acc: 0.4945\n",
      "Epoch 54/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.8018 - acc: 0.6529 - val_loss: 1.2802 - val_acc: 0.5091\n",
      "Epoch 55/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7832 - acc: 0.6595 - val_loss: 1.3102 - val_acc: 0.5027\n",
      "Epoch 56/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7772 - acc: 0.6608 - val_loss: 1.2501 - val_acc: 0.5155\n",
      "Epoch 57/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7655 - acc: 0.6655 - val_loss: 1.2468 - val_acc: 0.5064\n",
      "Epoch 58/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7642 - acc: 0.6688 - val_loss: 1.2332 - val_acc: 0.4991\n",
      "Epoch 59/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7537 - acc: 0.6725 - val_loss: 1.3550 - val_acc: 0.5109\n",
      "Epoch 60/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7430 - acc: 0.6772 - val_loss: 1.3255 - val_acc: 0.4936\n",
      "Epoch 61/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7324 - acc: 0.6808 - val_loss: 1.3494 - val_acc: 0.4836\n",
      "Epoch 62/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7214 - acc: 0.6884 - val_loss: 1.3105 - val_acc: 0.5027\n",
      "Epoch 63/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7204 - acc: 0.6769 - val_loss: 1.3721 - val_acc: 0.5109\n",
      "Epoch 64/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.7113 - acc: 0.6952 - val_loss: 1.3918 - val_acc: 0.4809\n",
      "Epoch 65/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6937 - acc: 0.7043 - val_loss: 1.3050 - val_acc: 0.5100\n",
      "Epoch 66/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6906 - acc: 0.7061 - val_loss: 1.3634 - val_acc: 0.5209\n",
      "Epoch 67/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6852 - acc: 0.7001 - val_loss: 1.2743 - val_acc: 0.5228\n",
      "Epoch 68/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6753 - acc: 0.7066 - val_loss: 1.4913 - val_acc: 0.5273\n",
      "Epoch 69/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6682 - acc: 0.7126 - val_loss: 1.3766 - val_acc: 0.5155\n",
      "Epoch 70/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6471 - acc: 0.7209 - val_loss: 1.4868 - val_acc: 0.4964\n",
      "Epoch 71/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6428 - acc: 0.7241 - val_loss: 1.6501 - val_acc: 0.4727\n",
      "Epoch 72/100\n",
      "3838/3838 [==============================] - 49s - loss: 0.6239 - acc: 0.7303 - val_loss: 1.3786 - val_acc: 0.5291\n",
      "Epoch 73/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6347 - acc: 0.7204 - val_loss: 1.4185 - val_acc: 0.5565\n",
      "Epoch 74/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.6048 - acc: 0.7371 - val_loss: 1.4606 - val_acc: 0.5282\n",
      "Epoch 75/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5910 - acc: 0.7439 - val_loss: 1.4486 - val_acc: 0.5383\n",
      "Epoch 76/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5912 - acc: 0.7439 - val_loss: 1.5575 - val_acc: 0.5273\n",
      "Epoch 77/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5754 - acc: 0.7551 - val_loss: 1.9113 - val_acc: 0.5009\n",
      "Epoch 78/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5803 - acc: 0.7520 - val_loss: 1.5234 - val_acc: 0.5155\n",
      "Epoch 79/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5629 - acc: 0.7548 - val_loss: 1.6404 - val_acc: 0.4964\n",
      "Epoch 80/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5567 - acc: 0.7590 - val_loss: 1.6685 - val_acc: 0.5009\n",
      "Epoch 81/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5422 - acc: 0.7676 - val_loss: 1.7207 - val_acc: 0.4973\n",
      "Epoch 82/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5403 - acc: 0.7632 - val_loss: 1.5303 - val_acc: 0.5319\n",
      "Epoch 83/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5514 - acc: 0.7634 - val_loss: 1.5192 - val_acc: 0.5419\n",
      "Epoch 84/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.5082 - acc: 0.7832 - val_loss: 1.6659 - val_acc: 0.5301\n",
      "Epoch 85/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4855 - acc: 0.7905 - val_loss: 1.7172 - val_acc: 0.5200\n",
      "Epoch 86/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4648 - acc: 0.7962 - val_loss: 1.8864 - val_acc: 0.5073\n",
      "Epoch 87/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4740 - acc: 0.7942 - val_loss: 1.9638 - val_acc: 0.4973\n",
      "Epoch 88/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4737 - acc: 0.7986 - val_loss: 1.8422 - val_acc: 0.5046\n",
      "Epoch 89/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4521 - acc: 0.8095 - val_loss: 1.8395 - val_acc: 0.5255\n",
      "Epoch 90/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4509 - acc: 0.8051 - val_loss: 2.0060 - val_acc: 0.5018\n",
      "Epoch 91/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4536 - acc: 0.8028 - val_loss: 1.8832 - val_acc: 0.5464\n",
      "Epoch 92/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4508 - acc: 0.8088 - val_loss: 1.9214 - val_acc: 0.5301\n",
      "Epoch 93/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4524 - acc: 0.8090 - val_loss: 2.3725 - val_acc: 0.5100\n",
      "Epoch 94/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4578 - acc: 0.8015 - val_loss: 1.8798 - val_acc: 0.5310\n",
      "Epoch 95/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4007 - acc: 0.8351 - val_loss: 2.0787 - val_acc: 0.5282\n",
      "Epoch 96/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.4130 - acc: 0.8304 - val_loss: 2.1669 - val_acc: 0.5109\n",
      "Epoch 97/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.3912 - acc: 0.8387 - val_loss: 2.0775 - val_acc: 0.5428\n",
      "Epoch 98/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.3866 - acc: 0.8338 - val_loss: 2.2144 - val_acc: 0.4882\n",
      "Epoch 99/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.3869 - acc: 0.8418 - val_loss: 1.9803 - val_acc: 0.5219\n",
      "Epoch 100/100\n",
      "3838/3838 [==============================] - 50s - loss: 0.3809 - acc: 0.8377 - val_loss: 2.2587 - val_acc: 0.5036\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(xtrain_sp, ytrain_sp, \n",
    "                 batch_size=100, nb_epoch=100, verbose=1, shuffle = True, \n",
    "                 validation_data=(xtest_sp, ytest_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
